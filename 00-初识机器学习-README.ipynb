{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 机器学习的感性认识\n",
    "\n",
    "这里区分出算法和机器学习之间的区别，\n",
    "* 算法：让机器去执行\n",
    "* 机器学习：让机器去学习\n",
    "\n",
    "举个例子：对于垃圾邮件的分辨问题。\n",
    "传统的计算机解决问题的思路是： 编写规则，定义“垃圾邮件”，让计算机执行，但是这样做有很多问题：\n",
    "* 对于很多问题，规则很难定义\n",
    "* 规则总是在不断变化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如下面的确定是猫还是狗的问题，人类能够很容易辨别左边是狗右边是猫，但是到底是什么规则让你认为左边是够右边是猫呢？其实这个规则很难回答的，也很难使用代码来编写这个规则，所以很多的领域都需要机器学习的方法来解决，也就是让机器自己去寻找这张图片的信息和她对应的是猫还是狗。\n",
    "\n",
    "![](img/initial/initial1.jpg)\n",
    "\n",
    "相同的还有人脸识别的领域，使用机器学习来解决的效果是很好的。再例如数字识别的领域，使用机器学习的方法来解决，在邮政系统领域有很大的运用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在下面的代码和笔记中将会涵盖：\n",
    "* 算法原理的学习\n",
    "* 部分算法底层的编写\n",
    "* sk-learn 机器学习库的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什么是机器学习\n",
    "\n",
    "![Machine Learning](img/isea3.png)\n",
    "\n",
    ">一般而言，我们需要就训练数据集作为机器学习算法的输入，以此来训练出一个模型；然后我们再往这个模型中输入样例，就可以得到预测值。\n",
    "fit：训练，拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 机器学习的基本任务（解决什么问题）\n",
    "\n",
    "* 分类\n",
    "    * 二分类（判断是猫是狗、信贷有无风险）\n",
    "    * 多分类（如数字识别、图像识别，围棋[可以转化为分类问题]）\n",
    "    * 多分类标签（如将一个图片既分到A标签下，又分到B标签下）\n",
    "* 回归 ：结果是一个连续的值，而非一个类别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 机器学习的哲学\n",
    "\n",
    "### 数据和算法之间的关系\n",
    "\n",
    "研究表明：数据量越大，准确率越高，当数据量大到一定的程度的时候，算法的准确率之间的差别并不大。\n",
    "\n",
    "所以目前的机器学习领域或者是人工智能领域，基本上都是数据驱动的，也即高度依赖数据的质量。包括：\n",
    "* 收集更多的数据\n",
    "* 提高数据质量\n",
    "* 提高数据的代表性\n",
    "* 研究更重要的特征\n",
    "\n",
    "ps:AlphaGo Zero例外，其数据是自己产生的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 奥卡姆剃刀原则\n",
    "\n",
    "简单的就是好的。也就是在选择什么算法的时候，遵循这样的一个原则。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 没有免费的午餐定理\n",
    "\n",
    "该定理指出：任意两个算法，他们的期望是相同的。\n",
    "\n",
    "也就是说A和B两个算法作用在所有的问题中，对于有的问题A优于B，另外一些问题B优于A，但是平均来说A和B是一样的。 但是具体到某个问题的时候，有些算法可能更好。\n",
    "\n",
    "所以在面对一个具体的问题的时候，尝试使用更多的算法进行对比试验是有必要的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 各种机器学习算法适用的场景\n",
    "\n",
    "#### KNN\n",
    "\n",
    "对于待判断的点，找到离它最近的几个数据点，根据它们的类型决定待判断点的类型。它的特点是完全跟着数据走，没有数学模型可言。\n",
    "\n",
    "\n",
    "适用情景：需要一个特别容易解释的模型的时候。比如需要向用户解释原因的推荐算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 逻辑回归\n",
    "回归方法的核心就是为函数找到最合适的参数，使得函数的值和样本的值最接近。例如线性回归(Linear regression)就是对于函数f(x)=ax+b，找到最合适的a,b。LR拟合的就不是线性函数了，它拟合的是一个概率学中的函数，f(x)的值这时候就反映了样本属于这个类的概率。\n",
    "\n",
    "适用情景：\n",
    "逻辑回归同样是很多分类算法的基础组件，它的好处是输出值自然地落在0到1之间，并且有概率意义。因为它本质上是一个线性的分类器，所以处理不好特征之间相关的情况。虽然效果一般，却胜在模型清晰，背后的概率学经得住推敲。它拟合出来的参数就代表了每一个特征(feature)对结果的影响。也是一个理解数据的好工具。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM\n",
    "\n",
    "SVM的核心思想就是找到不同类别之间的分界面，使得两类样本尽量落在面的两边，而且离分界面尽量远。\n",
    "\n",
    "最早的SVM是平面的，局限很大。但是利用核函数(kernel function)，我们可以把平面投射(mapping)成曲面，进而大大提高SVM的适用范围。提高之后的SVM同样被大量使用，在实际分类中展现了很优秀的正确率。\n",
    "\n",
    "\n",
    "适用情景：\n",
    "\n",
    "SVM在很多数据集上都有优秀的表现。相对来说，SVM尽量保持与样本间距离的性质导致它抗攻击的能力更强。**和随机森林一样，这也是一个拿到数据就可以先尝试一下的算法**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 决策树\n",
    "决策树的特点是它总是在沿着特征做切分。随着层层递进，这个划分会越来越细。虽然生成的树不容易给用户看，但是数据分析的时候，通过观察树的上层结构，能够对分类器的核心思路有一个直观的感受。举个简单的例子，当我们预测一个孩子的身高的时候，决策树的第一层可能是这个孩子的性别。男生走左边的树进行进一步预测，女生则走右边的树。这就说明性别对身高有很强的影响。\n",
    "\n",
    "适用情景：因为它能够基于特征(feature)选择，产生不同预测结果的清晰树状结构，数据分析师希望更好的理解手上的数据的时候往往可以使用决策树。同时它也是相对容易被攻击的分类器。这里的攻击是指人为的改变一些特征，使得分类器判断错误。常见于垃圾邮件躲避检测中。因为决策树最终在底层判断是基于单个条件的，攻击者往往只需要改变很少的特征就可以逃过监测。受限于它的简单性，决策树更大的用处是作为一些更有用的算法的基石。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 随机森林\n",
    "\n",
    "提到决策树就不得不提随机森林。顾名思义，森林就是很多树。严格来说，随机森林其实算是一种集成算法。它首先随机选取不同的特征(feature)和训练样本(training sample)，生成大量的决策树，然后综合这些决策树的结果来进行最终的分类。随机森林在现实分析中被大量使用，它相对于决策树，在准确性上有了很大的提升，同时一定程度上改善了决策树容易被攻击的特点。\n",
    "\n",
    "适用情景：数据维度相对低（几十维），同时对准确性有较高要求时。因为不需要很多参数调整就可以达到不错的效果，**基本上不知道用什么方法的时候都可以先试一下随机森林。**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习总结\n",
    "\n",
    "这里给出简要的总结，其中大部分内容在jupyter notebook中做详尽的描述。\n",
    "\n",
    "## 概述\n",
    "\n",
    "结合2019年公司数据挖掘的项目和成果，学习了机器学习基础的经典算法，和关于机器学习领域知识。主要涉及些数学知识，例如导数、偏导数、梯度，概率等；Python的一些库函数，例如Numpy，Matplotlib等；sk-learn机器学习库等。\n",
    "\n",
    "其中对以下的几个机器学习算法进行深入的学习：\n",
    "\n",
    "- K近邻\n",
    "- 简单线性回归\n",
    "- 多元线性回归\n",
    "- 主成分分析\n",
    "- 多项式回归\n",
    "- 逻辑回归\n",
    "- 支撑向量机\n",
    "- 决策树\n",
    "- 集成学习\n",
    "- 随机森林\n",
    "\n",
    "并没有包含专门领域的机器学习内容：如视觉领域、推荐引擎、NLP、时间序列等\n",
    "\n",
    "## 环境\n",
    "\n",
    "- 语言：Python3\n",
    "- 框架：Scikit-learn\n",
    "- 其他：Numpy，matplotlib...\n",
    "- IDE：Jupyter Notebook\n",
    "\n",
    "对于以上使用的是[Anaconda3](<https://www.anaconda.com/>)这个集成的机器学习环境，可以移步安装该环境（ps:非常容易安装），由于Jupyter notebook支持代码和markdown笔记的特性， 关于上述机器学习算法的代码、笔记都是使用Jupiter Notebook来记录。\n",
    "\n",
    "对于代码中使用的数据集都是来自于scikit-learn内置的数据集 。所有的代码和笔记都上传了Github ，地址为：[机器学习代码和笔记地址](<https://github.com/ifseayou/machine-learning>) ；可以本地安装好环境，把代码clone下来运行，将会更加清楚代码细节。\n",
    "\n",
    "如果你真的不想安装环境，但是想看看相关的代码和笔记，可以移步该项目在[GitHub的地址](<https://github.com/ifseayou/machine-learning>)，由于每个文件较大，加载出来可能得等待一会。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 对每个子模块的介绍\n",
    "\n",
    "### 01-Numpy数组和矩阵\n",
    "\n",
    "主要介绍\n",
    "\n",
    "- Numpy相对于Python中数组的区别，使用Numpy库中的数组是一个向量，可以进行向量运算。例如一些方法的使用，array、reshape、arange、linspace、random等\n",
    "- matplotlib 实现数据的可视化，主要是连线图、散点图\n",
    "- 对于sk-learn库中的数据集合的读取和简单的数据剪裁\n",
    "\n",
    "## 02-KNN\n",
    "\n",
    "KNN是机器学习领域最基本、最简单，但是效果却很好的算法，主要用来解决分类问题，而且天然的能够解决多分类问题。属于**监督学习**。\n",
    "\n",
    "主要介绍：\n",
    "\n",
    "- KNN的主要思想\n",
    "- 机器学习算法的三个要素，和机器学习的一般步骤\n",
    "- 样本的训练和测试数据集拆分理论\n",
    "- 分类准确度理论\n",
    "- 超参数理论，KNN中的超参数，K、权重\n",
    "- 计算距离的方式，欧拉距离、曼哈顿距离、明可夫斯基距离等\n",
    "- 网格搜索法寻找最优的超参数\n",
    "- 数据的归一化方法：最值归一化、均值方差归一化\n",
    "- sk-learn中 KNN的使用、Scaler的使用\n",
    "- KNN的缺点：效率、维度灾难，预测结果的不可解释性\n",
    "- KNN如何解决回归问题\n",
    "\n",
    "## 03-梯度下降、上升法\n",
    "\n",
    "梯度下降|上升法本身并不是一个机器学习算法，但是在机器学习领域有着非常重要的作用，两者都是用来求解目标函数的，其中梯度下降法用来求解损失函数的最小值，梯度上升法用来求解效用函数的最大值，监督学习。主要介绍了：\n",
    "\n",
    "- 梯度的数学意义\n",
    "- 梯度下降法求解简单线性归回的损失函数\n",
    "- 梯度下降法求解多元线性回回的损失函数\n",
    "- 梯度下降法的向量化运算\n",
    "\n",
    "\n",
    "## 04-简单线性回归\n",
    "\n",
    "线性回归是机器学习算法中最为基础的回归算法，主要用来解决回归问题，其思想和实现不仅简单，易于实现，而且其还是很多非线性模型的基础，其具有很好的解释性，**监督学习**。主要介绍了：\n",
    "\n",
    "- 简单线性回归的思想\n",
    "- 使用（交替）最小二乘法ASL，来求解简单线性回归的数学解，公式推导过程\n",
    "- 对线性回归算法评测：均方误差、均方根误差、最值误差。\n",
    "- 回归问题最好的评测方法：$R^2$\n",
    "\n",
    "\n",
    "\n",
    "## 05-多元线性回归\n",
    "\n",
    "简单线性回归的数据特征只有一个维度，多元线性回归有多个维度，也即支持数据的多个特征。**监督学习**。主要介绍了：\n",
    "\n",
    "- 多元线性回归的理论基础\n",
    "- 使用梯度下降法求解多元线性回归\n",
    "- 波士顿房产实例\n",
    "- 线性回归的可解释性、网格所有最优的超参数\n",
    "\n",
    "## 06- 主成分分析\n",
    "\n",
    "PCA，是统计领域中非常重要的方法，也是机器学习领域非常重要的算法，主要用于数据降维，去燥。非监督学习。主要介绍了\n",
    "\n",
    "- 什么是主成分，主成分的原理\n",
    "- 如何使用梯度上升法求取数据的主成分\n",
    "- 高维度数据 如何向低维度数据映射\n",
    "- sk-learn中PCA的使用和其在真实的数据集上的表现\n",
    "- 使用PCA对数据进行降维可视化\n",
    "- 使用PCA对数据进行降噪\n",
    "- 手写数字识别实例、minist数据案例\n",
    "\n",
    "## 07、08-多项式回归和模型泛化\n",
    "\n",
    "主要解决回归问题，适用于没有线性关系的数据，可以对非线性数据进行处理和预测。主要介绍：\n",
    "\n",
    "- 使用线性回归的思路解决非线性问题\n",
    "- sk-learn中的多项式回归和Pipeline\n",
    "- 过拟合和欠拟合\n",
    "- 模型泛化、模型复杂度和模型准确率之间的关系\n",
    "\n",
    "## 09-逻辑回归\n",
    "\n",
    "主要用来解决分类问题，是使用较多的算法，主要介绍了\n",
    "\n",
    "- 逻辑回归的理论基础\n",
    "- sigmod函数的特性\n",
    "- 逻辑回归的损失函数\n",
    "- 使用梯度下降法求解逻辑回归的目标函数\n",
    "- 逻辑回归的实现\n",
    "- 决策边界、决策边界的几何意义，KNN等算法的决策边界\n",
    "- 逻辑回归中使用多项式项\n",
    "- 使用正则化来抑制逻辑回归的过拟合\n",
    "- sk-learn中的逻辑回归使用，及其逻辑回归的超参数\n",
    "\n",
    "## 10-支撑向量机\n",
    "\n",
    "可以解决回归问题，也可以解决分类问题。主要介绍\n",
    "\n",
    "- 支撑向量机主要解决什么问题\n",
    "- Hard Margin SVM\n",
    "- Soft Margin SVM\n",
    "- 支撑向量的原理和推导过程\n",
    "- Soft Margin SVM 的正则化，抑制过拟合\n",
    "- sk-learn中SVM的使用（SVC）\n",
    "- SVM中使用多项式项，来处理非线性问题\n",
    "- 多项式核函数和SVM\n",
    "\n",
    "## 11、12-决策树\n",
    "\n",
    "非参数学习，主要解决分类问题，而且天然的可以解决多分类问题，也可以解决回归问题\n",
    "\n",
    "本质是每次选择一个最优的维度和一个最优的阈值，主要介绍了：\n",
    "\n",
    "- 决策树的原理\n",
    "- 使用信息熵寻找决策树的最优划分（也即使用信息熵来表征决策的误差）\n",
    "- 使用基尼系数寻找决策树的最优划分（也即使用基尼系数来表征决策的误差）\n",
    "- 决策树的超参数，和网格化搜索\n",
    "- 使用使用决策树解决回顾问题\n",
    "- sk-learn中决策树算法的使用\n",
    "\n",
    "\n",
    "\n",
    "## 13、14- 集成学习，随机森林\n",
    "\n",
    "主要解决分类问题。非参数学习，非监督学习。\n",
    "\n",
    "- 集成学习的原理\n",
    "- bagging和pasting\n",
    "- OOB\n",
    "- 随机森林和集成学习之间的关系\n",
    "\n",
    "## 最后\n",
    "\n",
    "\n",
    "**GitHub地址**：<https://github.com/ifseayou/machine-learning> 可以直接查看笔记和代码。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
